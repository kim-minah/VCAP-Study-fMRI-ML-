{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9209acd0-03ef-425d-8056-cc67a7546e7b",
   "metadata": {},
   "source": [
    "# Step 1: Feature Engineering & Model Comparisons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b1ae7f-659f-4ca8-b9e8-ba6f4c0d8cca",
   "metadata": {},
   "source": [
    "### This first part does the following\n",
    "- Configure project paths and load input files (CSV/PKL).\n",
    "- Prepare connectivity features from time series data (optionally can choose to select nodes from single network).\n",
    "- Apply participant exclusion. \n",
    "- Residualize features on covariates (e.g., Age/Sex) to remove nuisance variance.\n",
    "- Standardize features within a modeling pipeline.\n",
    "- Define and run ML models (Ridge/Lasso/Elastic Net) with cross-validation and hyperparameter tuning.\n",
    "- Save results (RÂ², MAE, RMSE, Pearson and Spearman correlations).\n",
    "- Compare models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65f643e2-136e-4a85-befd-96d94db56473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "import os, sys, json, math, random, pathlib, itertools, functools, warnings\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import pickle\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "# Set directory that has the fmri data\n",
    "input_dir = 'C:/Users/mk7kc/Desktop/connectivity/vcap/vcap_Shen/dat'\n",
    "os.chdir(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81099a6-1dc1-4f4c-bd5a-067c3c58cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify output dir (make if it doesn't already exist)\n",
    "output_dir=''\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725f6029-48c3-413d-b6b2-57e816923968",
   "metadata": {},
   "source": [
    "## Read in characterization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eb10e7-67f8-4048-8d7e-82e4b6a89cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in characterization data, remove participants with too high of a mean FD value (motion artifact too high)\n",
    "# See exclusion.txt for more info\n",
    "char_dir = 'C:/Users/mk7kc/Desktop/connectivity/vcap/vcap_MSDL/char_data/'\n",
    "char_data = pd.read_csv('C:/Users/mk7kc/Desktop/connectivity/vcap/vcap_MSDL/char_data/char.csv')\n",
    "exclude = [10, 43, 59, 78, 80, 83, 93]\n",
    "\n",
    "# Demographic covariates\n",
    "age = np.delete(char_data['Age'].values, exclude)\n",
    "sex = np.delete(char_data['Sex'].values, exclude)\n",
    "\n",
    "# Create function for faster extraction\n",
    "def load_char(file, column):\n",
    "    data = pd.read_csv(file)\n",
    "    column_vals = data[column].values\n",
    "    return np.delete(column_vals, exclude)\n",
    "\n",
    "# Social Network metrics\n",
    "SNComposite = load_char(\n",
    "    os.path.join(char_dir, 'faOut1mlPromax_tenBergescores.csv'), 'ML1'\n",
    ")\n",
    "\n",
    "AnticipatedSupport = load_char(\n",
    "    os.path.join(char_dir, 'socialnetwork_subscale_scores.csv'), 'PercSupp_Anticip'\n",
    ")\n",
    "\n",
    "SN_metrics = np.column_stack([SNComposite, AnticipatedSupport])\n",
    "\n",
    "\n",
    "# Cognitive metrics\n",
    "vocab = load_char(\n",
    "    os.path.join(char_dir, 'vcap_cog_MK.csv'), 'vocab'\n",
    ")\n",
    "\n",
    "proc_speed = load_char(\n",
    "    os.path.join(char_dir, 'vcap_cog_MK.csv'), 'proc_speed'\n",
    ")\n",
    "\n",
    "mem = load_char(\n",
    "    os.path.join(char_dir, 'vcap_cog_MK.csv'), 'memory'\n",
    ")\n",
    "\n",
    "cog_metrics = np.column_stack([vocab, proc_speed, mem])  \n",
    "\n",
    "\n",
    "variables={\"Social Support\":SN_metrics[:,0],\n",
    "           \"Anticipated Support\":SN_metrics[:,1],\n",
    "          \"Vocab\":cog_metrics[:,0],\n",
    "          \"Processing Speed\":cog_metrics[:,1],\n",
    "          \"Memory\":cog_metrics[:,2],\n",
    "           \"Age\":age,\n",
    "           \"Sex\":sex,\n",
    "          }\n",
    "df=pd.DataFrame(variables)\n",
    "df.shape()\n",
    "# Standardize values\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df),columns=df.columns)\n",
    "\n",
    "# Visualize pairplots and distributions\n",
    "_ = sns.pairplot(df_scaled, vars=df.columns,kind=\"reg\", diag_kind=\"kde\",height=1.7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14254be8-86ba-4da1-b13b-91fb4ce93101",
   "metadata": {},
   "source": [
    "### Choose fMRI Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607ce2f-9ea9-4e66-bc29-fa7ee7072030",
   "metadata": {},
   "source": [
    "#### Option 1: Dictionary Learning output from high performance cluster processing of CPAC 4D standard to func outputs\n",
    "Spatial maps generated via map learning algorithm based on spatial component sparsity then timeseries extracted for connectivty measures. To see model and gridsearch parameters see VCAP_CPAC folder for python scripts run on HPC for network & region extraction & connectome generation. **Note: participants that needed to be excluded were already excluded in the HPC step, so no need to do it here.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57046f63-fcb9-4280-9b2e-45e06b05ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in partial correlation data, use upper triangle data for features, Fishers Z transform \n",
    "fc_noGM_noLP = np.load('12comp_pcorr_noGM_noLP.npy')\n",
    "fc_noGM_noLP = fc_noGM_noLP.reshape(92,71,71)\n",
    "fc_noGM_noLP_feature=[]\n",
    "for i in range(0,92):\n",
    "    fv=fc_noGM_noLP[i][np.triu_indices(71, k = 1)] #k=1 excludes diagonal values\n",
    "    fc_noGM_noLP_feature.append(fv)\n",
    "x = np.arctanh(fc_noGM_noLP_feature) # 66 features\n",
    "\n",
    "# Plot components using output from the best estimator model's components_img_\n",
    "dictlearning_components_img = nib.load(input_dir+'.nii.gz')\n",
    "\n",
    "# Plot atlas\n",
    "# plot_prob_atlas(dictlearning_components_img,\n",
    "#                 display_mode='z')\n",
    "# plot_prob_atlas(dictlearning_components_img,\n",
    "#                display_mode='x')\n",
    "# plot_prob_atlas(dictlearning_components_img,\n",
    "#                display_mode='y')\n",
    "\n",
    "# Save images\n",
    "#plot_prob_atlas(dictlearning_components_img,output_file=output_dir+'vcap_All_DictLearning_Components.png')\n",
    "#plot_prob_atlas(dictlearning_components_img,\n",
    "#                display_mode='x',output_file=output_dir+'vcap_All_DictLearning_Components_x.svg')\n",
    "\n",
    "# Plot map for each component separately and save it into visualizations folder\n",
    "# for i, cur_img in enumerate(iter_img(dictlearning_components_img)):\n",
    "#     vis_name = os.path.join(output_dir,\"Comp_y_\"+str(i+1))\n",
    "#     plot_stat_map(cur_img, display_mode=\"xz\", title=\"Comp\"+str(i+1),\n",
    "#                   colorbar=True,output_file = vis_name) \n",
    "    \n",
    "# Examine explained variance using best estimator model's scores   \n",
    "scores = np.load(os.path.join(input_dir,'scores_12comp_220511_92participants_noGM_noLP.npy'))\n",
    "print('My components explain %s perct. of the variance in the dataset' % str(round(np.sum(scores)*100,2)))\n",
    "\n",
    "# Plot the explained variances per component\n",
    "plt.figure(figsize=(10, 10))\n",
    "numbers = np.arange(1,13)\n",
    "plt.barh(numbers,scores)\n",
    "plt.ylabel('Component #', size=30)\n",
    "plt.xlabel('Explained Variance Ratio', size=30)\n",
    "plt.yticks(np.arange(1,13))\n",
    "plt.tick_params(axis='x', labelsize=20)\n",
    "plt.tick_params(axis='y', labelsize=10)\n",
    "plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(output_dir,\"ExplainedVariance_80comp.svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b03253-52f3-4e49-b592-c1538650ba13",
   "metadata": {},
   "source": [
    "#### Option 2: Atlas output from fMRIPrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b155c017-d01a-4a8b-9a84-7ba4a17f5c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (subjects, features): (92, 35778)\n"
     ]
    }
   ],
   "source": [
    "# Specify atlas name in files\n",
    "atlas_label = 'shen_atlas' # e.g. 'shen_atlas'\n",
    "task_label = 'rest_1' # e.g. 'rest_1'\n",
    "with open(f'vcap_{atlas_label}_ts_{task_label}.pkl', 'rb') as handle:\n",
    "    fcdat = pickle.load(handle) \n",
    "    \n",
    "# (Optional) Only extract nodes from certain networks, e.g. DMN network in Shen atlas\n",
    "network_use = False\n",
    "labels = pd.read_csv('shen_268_parcellation_networklabels.csv')\n",
    "network_indices = labels.loc[labels['Network'] == 3].index # DMN = 3\n",
    "\n",
    "# Extract time series from participants used for final analysis (remove those who meet exclusion criteria)\n",
    "# List of participant IDs to exclude\n",
    "exclude_id = char_data.iloc[exclude,0].values\n",
    "exclude_id_str = {f\"{n:03d}\" for n in exclude_id}\n",
    "\n",
    "# Get participant IDs\n",
    "ids = sorted(fcdat.keys())\n",
    "include_id = [include for include in ids if include not in exclude_id_str]\n",
    "if network_use:\n",
    "    ts = [fcdat[include][:, network_indices] for include in include_id]\n",
    "else:\n",
    "    ts = [fcdat[include] for include in include_id]             \n",
    "\n",
    "# Connectivity\n",
    "conn = ConnectivityMeasure(kind='partial correlation', vectorize=True, discard_diagonal=True)\n",
    "x = np.arctanh(conn.fit_transform(ts))  # shape: (n_subjects, n_features)\n",
    "print(\"X shape (subjects, features):\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaca735-5fdf-4b04-9e3c-a5f328a90165",
   "metadata": {},
   "source": [
    "## Regress out covariates:\n",
    "A linear regression analysis was performed at every feature to remove the effects of covariates. The residuals of this regression were then substituted for the feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965033c6-289a-4f5c-bc63-61a75ccb1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar = ['Age', 'Sex'] # add any other demo or covariates as needed\n",
    "df_scaled_const = sm.add_constant(df_scaled[covar].values)\n",
    "\n",
    "# # For loop version (takes longer)\n",
    "# x_res = np.empty_like(x)\n",
    "# for i in range(x.shape[1]):\n",
    "#     x_res[:,i] = sm.OLS(x[:,i],df_scaled2).fit().resid \n",
    "\n",
    "# Vectorized version\n",
    "M = np.eye(df_scaled_const.shape[0]) - df_scaled_const @ np.linalg.pinv(df_scaled_const)\n",
    "x_res = M @ x    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ff368a-dfd0-4d4c-b69b-a783ebf4e7d7",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd0a47-7e2a-4f6e-adc1-75cd50b32b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ML(X, Y, target_names, model, perm=1000, test_size=0.2,\n",
    "           cv_splits=5, cv_repeats=5, random_seed=1,\n",
    "           date_prefix=None,descriptor=None):\n",
    "    \"\"\"\n",
    "    X: residualized connectivity data\n",
    "    Y: predictor vars\n",
    "    model: choose between ridge, lasso, elasticnet\n",
    "    \"\"\"\n",
    "    model = model.lower()\n",
    "    if model not in {'ridge','lasso','elasticnet'}:\n",
    "        raise ValueError(\"check spelling of model type\")\n",
    "    n, n_targets = Y.shape\n",
    "\n",
    "    # CV for inner tuning\n",
    "    cv = RepeatedKFold(n_splits=cv_splits, n_repeats=cv_repeats, random_state=random_seed)\n",
    "\n",
    "    # Search spaces\n",
    "    alphas_ridge = np.linspace(4, 15, 50)\n",
    "    alphas_lasso = np.linspace(0.1, .2, 100)\n",
    "    alphas_e = np.linspace(0.2, 0.5, 20)\n",
    "    l1_ratios = np.linspace(0.7, 1, 10) #  pure Ridge regression (l1_ratio=0) to pure Lasso regression (l1_ratio=1)\n",
    "    \n",
    "    # Arrays to store results\n",
    "    r2 = np.full((perm, n_targets), np.nan)\n",
    "    mae = np.full((perm, n_targets), np.nan)\n",
    "    rmse = np.full((perm, n_targets), np.nan)\n",
    "    pearson = np.full((perm, n_targets), np.nan)\n",
    "    spearman = np.full((perm, n_targets), np.nan)\n",
    "\n",
    "    # Optimized hyperparameters\n",
    "    opt_alpha = np.full((perm, n_targets), np.nan)\n",
    "    opt_l1 = np.full((perm, n_targets), np.nan)\n",
    "\n",
    "    for p in range(perm):\n",
    "        rs = np.random.RandomState(random_seed + p)\n",
    "        idx = np.arange(n).copy()\n",
    "        rs.shuffle(idx)\n",
    "        n_test = int(np.ceil(n * test_size))\n",
    "        test_idx, train_idx = idx[:n_test], idx[n_test:]\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "\n",
    "        for t in range(n_targets):\n",
    "            y = Y[:, t]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            # Build estimator\n",
    "            if model == 'ridge':\n",
    "                est = RidgeCV(alphas=alphas_ridge, cv=cv, scoring='r2')\n",
    "                est_name = 'ridgecv'\n",
    "            elif model == 'lasso':\n",
    "                est = LassoCV(alphas=alphas_lasso, cv=cv, n_jobs=-1,\n",
    "                              random_state=random_seed + p, max_iter=1e7)\n",
    "                est_name = 'lassocv'\n",
    "            elif model == 'elasticnet':\n",
    "                est = ElasticNetCV(alphas=alphas_e, l1_ratio=l1_ratios, cv=cv, n_jobs=-1,\n",
    "                                   random_state=random_seed + p, max_iter=1e7)\n",
    "                est_name = 'elasticnetcv'\n",
    "                \n",
    "            pipe = Pipeline([\n",
    "                ('sc', StandardScaler()),\n",
    "                (est_name, est)\n",
    "            ])\n",
    "\n",
    "            pipe.fit(X_train, y_train)\n",
    "            yhat = pipe.predict(X_test)\n",
    "\n",
    "            # Metrics\n",
    "            r2[p, t]   = r2_score(y_test, yhat)\n",
    "            mae[p, t]  = mean_absolute_error(y_test, yhat)\n",
    "            rmse[p, t] = mean_squared_error(y_test, yhat, squared=False)\n",
    "            pearson[p, t]  = np.corrcoef(y_test, yhat)[1, 0] if y_test.std() > 0 else np.nan\n",
    "            rho, _ = spearmanr(y_test, yhat)\n",
    "            spearman[p, t] = rho if y_test.std() > 0 else np.nan\n",
    "            \n",
    "            # Hyperparameters\n",
    "            result = pipe.named_steps[est_name]\n",
    "            if hasattr(result, 'alpha_'): opt_alpha[p, t] = result.alpha_\n",
    "            if hasattr(result, 'l1_ratio_'): opt_l1[p, t] = result.l1_ratio_\n",
    "\n",
    "        if (p + 1) % max(1, perm // 5) == 0:\n",
    "            print(f\"{p+1}/{perm} permutations\")\n",
    "\n",
    "    if date_prefix and descriptor:\n",
    "        # Save function\n",
    "        def save(outcome, array):\n",
    "            np.savetxt(os.path.join(output_dir, f'{date_prefix}_{descriptor}_{outcome}.csv'), array, delimiter=',')\n",
    "\n",
    "        save('r2', r2)\n",
    "        save('mae', mae)\n",
    "        save('rmse', rmse)\n",
    "        save('pearson', pearson)\n",
    "        save('spearman', spearman)\n",
    "        save('alpha', opt_alpha)\n",
    "        if model == 'elasticnet':\n",
    "            save('l1_ratio', opt_l1)\n",
    "\n",
    "    return {\n",
    "        'r2': r2, 'mae': mae, 'rmse': rmse, 'pearson': pearson, 'spearman': spearman,\n",
    "        'alpha': opt_alpha, 'l1_ratio': opt_l1 if model == 'elasticnet' else None,\n",
    "        'targets': target_names, 'model': model,\n",
    "        'config': {'perm': perm, 'test_size': test_size, 'cv_splits': cv_splits, 'cv_repeats': cv_repeats}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dda6b57-d00d-40b5-b4a0-189f563b5aaf",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf0f84-c6f3-42b3-be2a-32b14dffc656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final Y dataframe\n",
    "Y = df_scaled.drop(columns=[\"Age\", \"Sex\"]).values\n",
    "y_metric_names = df_scaled.drop(columns=[\"Age\", \"Sex\"]).columns.tolist()\n",
    "\n",
    "# Run model\n",
    "run_ML(x_res, Y, y_metric_names, model='elasticnet', perm=1000,\n",
    "                    test_size=0.2, cv_splits=5, cv_repeats=5, random_seed=1,\n",
    "                    date_prefix='230301',descriptor='elasticnet_partialcorr_DMN' if use_dmn_only else 'elasticnet_partialcorr_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cac1f60-43e1-46aa-ae85-b4cf03f00b2a",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c94fe7a1-ce29-484f-b799-1993b9abb74d",
   "metadata": {},
   "source": [
    "means = np.nanmean(results['pearson'], axis=0)\n",
    "stds  = np.nanstd(results['pearson'], axis=0)\n",
    "names = results['targets']\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.errorbar(range(len(names)), means, yerr=stds, fmt='o-', capsize=4)\n",
    "plt.xticks(range(len(names)), names, rotation=30, ha='right')\n",
    "plt.ylabel(\"Pearson (mean Â± SD)\", fontsize=14)\n",
    "plt.title(f\"Across {results['config']['perm']} permutations â {results['model']}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot first target's Pearson across permutations\n",
    "target_ix = 0\n",
    "series = results['pearson'][:, target_ix]\n",
    "plt.figure(figsize=(12,6)); plt.plot(series); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801dbb72-33e3-4066-95b2-3406bb54d167",
   "metadata": {},
   "source": [
    "## Compare the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d9f1c-3974-456c-9c2d-aa61108efefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model comparison ===\n",
    "from collections import OrderedDict\n",
    "\n",
    "seed = 1\n",
    "perm = 500  \n",
    "results_by_model = OrderedDict()\n",
    "for m in [\"ridge\", \"lasso\", \"elasticnet\"]:\n",
    "    results_by_model[m] = run_ML(\n",
    "        X_res, Y, target_names=y_metric_names, model=m,\n",
    "        perm=perm, test_size=0.2, cv_splits=5, cv_repeats=5,\n",
    "        random_seed=seed)\n",
    "    \n",
    "# This following part I got ChatGPT to clean up my code to be much more efficient\n",
    "def summarize_metric(metric_name: str):\n",
    "    rows = []\n",
    "    for m, res in results_by_model.items():\n",
    "        mat = res[metric_name] \n",
    "        rows.append(pd.DataFrame({\n",
    "            \"model\": m,\n",
    "            \"target\": res[\"targets\"],\n",
    "            \"mean\": np.nanmean(mat, axis=0),\n",
    "            \"std\":  np.nanstd(mat, axis=0)\n",
    "        }))\n",
    "    return pd.concat(rows, ignore_index=True)\n",
    "\n",
    "sum_r2       = summarize_metric(\"r2\")\n",
    "sum_mae      = summarize_metric(\"mae\")\n",
    "sum_rmse     = summarize_metric(\"rmse\")\n",
    "sum_pearson  = summarize_metric(\"pearson\")\n",
    "sum_spearman = summarize_metric(\"spearman\")\n",
    "\n",
    "metric_to_pick = \"r2\"  # or \"pearson\"\n",
    "summary = {\"r2\": sum_r2, \"pearson\": sum_pearson, \"mae\": sum_mae, \"rmse\": sum_rmse, \"spearman\": sum_spearman}[metric_to_pick]\n",
    "\n",
    "best_per_target = (\n",
    "    summary.sort_values([\"target\", \"mean\"], ascending=[True, False])\n",
    "           .groupby(\"target\", as_index=False)\n",
    "           .first()[[\"target\", \"model\", \"mean\", \"std\"]]\n",
    "           .rename(columns={\"mean\": f\"{metric_to_pick}_mean\", \"std\": f\"{metric_to_pick}_std\"})\n",
    ")\n",
    "\n",
    "print(\"\\nBest model per target (by {}):\".format(metric_to_pick))\n",
    "print(best_per_target.to_string(index=False))\n",
    "\n",
    "# 4) Quick visualization: bar chart of mean Â± SD for your metric\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, tgt in enumerate(y_metric_names):\n",
    "    df_t = summary[summary[\"target\"] == tgt]\n",
    "    plt.errorbar(\n",
    "        x=np.arange(len(df_t)) + i*(0.04),  # small offset per target to reduce overlap\n",
    "        y=df_t[\"mean\"], yerr=df_t[\"std\"],\n",
    "        fmt=\"o-\", capsize=4, label=f\"{tgt}\" if i == 0 else None\n",
    "    )\n",
    "plt.xticks(\n",
    "    ticks=np.arange(len(df_t)), \n",
    "    labels=df_t[\"model\"].values, \n",
    "    rotation=0\n",
    ")\n",
    "plt.ylabel(f\"{metric_to_pick.capitalize()} (mean Â± SD)\")\n",
    "plt.title(f\"Model comparison across {perm} permutations\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
