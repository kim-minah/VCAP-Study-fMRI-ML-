{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9209acd0-03ef-425d-8056-cc67a7546e7b",
   "metadata": {},
   "source": [
    "# Step 2: Generate Empirical Null Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65f643e2-136e-4a85-befd-96d94db56473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "import os, sys, json, math, random, pathlib, itertools, functools, warnings\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import pickle\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "# Set directory that has the fmri data\n",
    "input_dir = 'C:/Users/mk7kc/Desktop/connectivity/vcap/vcap_Shen/dat'\n",
    "os.chdir(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81099a6-1dc1-4f4c-bd5a-067c3c58cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify output dir (make if it doesn't already exist)\n",
    "output_dir=''\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725f6029-48c3-413d-b6b2-57e816923968",
   "metadata": {},
   "source": [
    "## Read in characterization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eb10e7-67f8-4048-8d7e-82e4b6a89cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in characterization data, remove participants with too high of a mean FD value (motion artifact too high)\n",
    "# See exclusion.txt for more info\n",
    "char_dir = 'C:/Users/mk7kc/Desktop/connectivity/vcap/vcap_MSDL/char_data/'\n",
    "char_data = pd.read_csv('C:/Users/mk7kc/Desktop/connectivity/vcap/vcap_MSDL/char_data/char.csv')\n",
    "exclude = [10, 43, 59, 78, 80, 83, 93]\n",
    "\n",
    "# Demographic covariates\n",
    "age = np.delete(char_data['Age'].values, exclude)\n",
    "sex = np.delete(char_data['Sex'].values, exclude)\n",
    "\n",
    "# Create function for faster extraction\n",
    "def load_char(file, column):\n",
    "    data = pd.read_csv(file)\n",
    "    column_vals = data[column].values\n",
    "    return np.delete(column_vals, exclude)\n",
    "\n",
    "# Social Network metrics\n",
    "SNComposite = load_char(\n",
    "    os.path.join(char_dir, 'faOut1mlPromax_tenBergescores.csv'), 'ML1'\n",
    ")\n",
    "\n",
    "AnticipatedSupport = load_char(\n",
    "    os.path.join(char_dir, 'socialnetwork_subscale_scores.csv'), 'PercSupp_Anticip'\n",
    ")\n",
    "\n",
    "SN_metrics = np.column_stack([SNComposite, AnticipatedSupport])\n",
    "\n",
    "\n",
    "# Cognitive metrics\n",
    "vocab = load_char(\n",
    "    os.path.join(char_dir, 'vcap_cog_MK.csv'), 'vocab'\n",
    ")\n",
    "\n",
    "proc_speed = load_char(\n",
    "    os.path.join(char_dir, 'vcap_cog_MK.csv'), 'proc_speed'\n",
    ")\n",
    "\n",
    "mem = load_char(\n",
    "    os.path.join(char_dir, 'vcap_cog_MK.csv'), 'memory'\n",
    ")\n",
    "\n",
    "cog_metrics = np.column_stack([vocab, proc_speed, mem])  \n",
    "\n",
    "\n",
    "variables={\"Social Support\":SN_metrics[:,0],\n",
    "           \"Anticipated Support\":SN_metrics[:,1],\n",
    "          \"Vocab\":cog_metrics[:,0],\n",
    "          \"Processing Speed\":cog_metrics[:,1],\n",
    "          \"Memory\":cog_metrics[:,2],\n",
    "           \"Age\":age,\n",
    "           \"Sex\":sex,\n",
    "          }\n",
    "df=pd.DataFrame(variables)\n",
    "df.shape()\n",
    "# Standardize values\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df),columns=df.columns)\n",
    "\n",
    "# Visualize pairplots and distributions\n",
    "_ = sns.pairplot(df_scaled, vars=df.columns,kind=\"reg\", diag_kind=\"kde\",height=1.7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14254be8-86ba-4da1-b13b-91fb4ce93101",
   "metadata": {},
   "source": [
    "### Choose fMRI Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607ce2f-9ea9-4e66-bc29-fa7ee7072030",
   "metadata": {},
   "source": [
    "#### Option 1: Dictionary Learning output from high performance cluster processing of CPAC 4D standard to func outputs\n",
    "Spatial maps generated via map learning algorithm based on spatial component sparsity then timeseries extracted for connectivty measures. To see model and gridsearch parameters see VCAP_CPAC folder for python scripts run on HPC for network & region extraction & connectome generation. **Note: participants that needed to be excluded were already excluded in the HPC step, so no need to do it here.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57046f63-fcb9-4280-9b2e-45e06b05ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in partial correlation data, use upper triangle data for features, Fishers Z transform \n",
    "fc_noGM_noLP = np.load('12comp_pcorr_noGM_noLP.npy')\n",
    "fc_noGM_noLP = fc_noGM_noLP.reshape(92,71,71)\n",
    "fc_noGM_noLP_feature=[]\n",
    "for i in range(0,92):\n",
    "    fv=fc_noGM_noLP[i][np.triu_indices(71, k = 1)] #k=1 excludes diagonal values\n",
    "    fc_noGM_noLP_feature.append(fv)\n",
    "x = np.arctanh(fc_noGM_noLP_feature) # 66 features\n",
    "\n",
    "# Plot components using output from the best estimator model's components_img_\n",
    "dictlearning_components_img = nib.load(input_dir+'.nii.gz')\n",
    "\n",
    "# Plot atlas\n",
    "# plot_prob_atlas(dictlearning_components_img,\n",
    "#                 display_mode='z')\n",
    "# plot_prob_atlas(dictlearning_components_img,\n",
    "#                display_mode='x')\n",
    "# plot_prob_atlas(dictlearning_components_img,\n",
    "#                display_mode='y')\n",
    "\n",
    "# Save images\n",
    "#plot_prob_atlas(dictlearning_components_img,output_file=output_dir+'vcap_All_DictLearning_Components.png')\n",
    "#plot_prob_atlas(dictlearning_components_img,\n",
    "#                display_mode='x',output_file=output_dir+'vcap_All_DictLearning_Components_x.svg')\n",
    "\n",
    "# Plot map for each component separately and save it into visualizations folder\n",
    "# for i, cur_img in enumerate(iter_img(dictlearning_components_img)):\n",
    "#     vis_name = os.path.join(output_dir,\"Comp_y_\"+str(i+1))\n",
    "#     plot_stat_map(cur_img, display_mode=\"xz\", title=\"Comp\"+str(i+1),\n",
    "#                   colorbar=True,output_file = vis_name) \n",
    "    \n",
    "# Examine explained variance using best estimator model's scores   \n",
    "scores = np.load(os.path.join(input_dir,'scores_12comp_220511_92participants_noGM_noLP.npy'))\n",
    "print('My components explain %s perct. of the variance in the dataset' % str(round(np.sum(scores)*100,2)))\n",
    "\n",
    "# Plot the explained variances per component\n",
    "plt.figure(figsize=(10, 10))\n",
    "numbers = np.arange(1,13)\n",
    "plt.barh(numbers,scores)\n",
    "plt.ylabel('Component #', size=30)\n",
    "plt.xlabel('Explained Variance Ratio', size=30)\n",
    "plt.yticks(np.arange(1,13))\n",
    "plt.tick_params(axis='x', labelsize=20)\n",
    "plt.tick_params(axis='y', labelsize=10)\n",
    "plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(output_dir,\"ExplainedVariance_80comp.svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b03253-52f3-4e49-b592-c1538650ba13",
   "metadata": {},
   "source": [
    "#### Option 2: Atlas output from fMRIPrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b155c017-d01a-4a8b-9a84-7ba4a17f5c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (subjects, features): (92, 35778)\n"
     ]
    }
   ],
   "source": [
    "# Specify atlas name in files\n",
    "atlas_label = 'shen_atlas' # e.g. 'shen_atlas'\n",
    "task_label = 'rest_1' # e.g. 'rest_1'\n",
    "with open(f'vcap_{atlas_label}_ts_{task_label}.pkl', 'rb') as handle:\n",
    "    fcdat = pickle.load(handle) \n",
    "    \n",
    "# (Optional) Only extract nodes from certain networks, e.g. DMN network in Shen atlas\n",
    "network_use = False\n",
    "labels = pd.read_csv('shen_268_parcellation_networklabels.csv')\n",
    "network_indices = labels.loc[labels['Network'] == 3].index # DMN = 3\n",
    "\n",
    "# Extract time series from participants used for final analysis (remove those who meet exclusion criteria)\n",
    "# List of participant IDs to exclude\n",
    "exclude_id = char_data.iloc[exclude,0].values\n",
    "exclude_id_str = {f\"{n:03d}\" for n in exclude_id}\n",
    "\n",
    "# Get participant IDs\n",
    "ids = sorted(fcdat.keys())\n",
    "include_id = [include for include in ids if include not in exclude_id_str]\n",
    "if network_use:\n",
    "    ts = [fcdat[include][:, network_indices] for include in include_id]\n",
    "else:\n",
    "    ts = [fcdat[include] for include in include_id]             \n",
    "\n",
    "# Connectivity\n",
    "conn = ConnectivityMeasure(kind='partial correlation', vectorize=True, discard_diagonal=True)\n",
    "x = np.arctanh(conn.fit_transform(ts))  # shape: (n_subjects, n_features)\n",
    "print(\"X shape (subjects, features):\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaca735-5fdf-4b04-9e3c-a5f328a90165",
   "metadata": {},
   "source": [
    "## Regress out covariates:\n",
    "A linear regression analysis was performed at every feature to remove the effects of covariates. The residuals of this regression were then substituted for the feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965033c6-289a-4f5c-bc63-61a75ccb1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar = ['Age', 'Sex'] # add any other demo or covariates as needed\n",
    "df_scaled_const = sm.add_constant(df_scaled[covar].values)\n",
    "\n",
    "# # For loop version (takes longer)\n",
    "# x_res = np.empty_like(x)\n",
    "# for i in range(x.shape[1]):\n",
    "#     x_res[:,i] = sm.OLS(x[:,i],df_scaled2).fit().resid \n",
    "\n",
    "# Vectorized version\n",
    "M = np.eye(df_scaled_const.shape[0]) - df_scaled_const @ np.linalg.pinv(df_scaled_const)\n",
    "x_res = M @ x    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ff368a-dfd0-4d4c-b69b-a783ebf4e7d7",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd0a47-7e2a-4f6e-adc1-75cd50b32b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where you saved your optimized parameters\n",
    "input_dir = '' \n",
    "if input_dir:\n",
    "    os.chdir(input_dir)\n",
    "\n",
    "def read_opt(date_prefix,descriptor,model):\n",
    "    opt_alpha = np.loadtxt(f\"{date_prefix}_{descriptor}_alpha.csv\", delimiter=\",\")\n",
    "    opt_l1 = np.loadtxt(f\"{date_prefix}_{descriptor}_l1_ratio.csv\", delimiter=\",\") if model == 'elasticnet' else None\n",
    "    # In case I only had one y target, cause then opt_alpha will be 1D array with shape (p,). \n",
    "    if opt_alpha.ndim == 1:\n",
    "        opt_alpha = opt_alpha.reshape(-1, 1) # make sure it's 2D\n",
    "    if (opt_l1 is not None) and opt_l1.ndim == 1:\n",
    "        opt_l1 = opt_l1.reshape(-1, 1) # make sure it's 2D\n",
    "    return opt_alpha,opt_l1\n",
    "\n",
    "def run_Null(X, Y, target_names, model, perm=1000, test_size=0.2,random_seed=1,\n",
    "           date_prefix=None, descriptor=None):\n",
    "    \"\"\"\n",
    "    X: residualized connectivity data\n",
    "    Y: predictor vars\n",
    "    model: choose between ridge, lasso, elasticnet\n",
    "    \"\"\"\n",
    "    model = model.lower()\n",
    "    if model not in {'ridge','lasso','elasticnet'}:\n",
    "        raise ValueError(\"check spelling of model type\")\n",
    "    n, n_targets = Y.shape\n",
    "    \n",
    "    # Load saved hyperparameters\n",
    "    opt_alpha, opt_l1 = read_opt(date_prefix,descriptor,model)\n",
    "\n",
    "    # Arrays to store results\n",
    "    r2 = np.full((perm, n_targets), np.nan)\n",
    "    mae = np.full((perm, n_targets), np.nan)\n",
    "    rmse = np.full((perm, n_targets), np.nan)\n",
    "    pearson = np.full((perm, n_targets), np.nan)\n",
    "    spearman = np.full((perm, n_targets), np.nan)\n",
    "\n",
    "    for p in range(perm):\n",
    "        rs = np.random.RandomState(random_seed + p)\n",
    "        idx = np.arange(n).copy()\n",
    "        rs.shuffle(idx)\n",
    "        n_test = int(np.ceil(n * test_size))\n",
    "        test_idx, train_idx = idx[:n_test], idx[n_test:]\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "\n",
    "        for t in range(n_targets):\n",
    "            y = Y[:, t]\n",
    "            y_train, y_test = y[train_idx].copy(), y[test_idx] # make copy so that each shuffle is shuffle of original true y and not a shuffle of a shuffle (narrows effective space of randomizations and biases the null)\n",
    "            rs.shuffle(y_train)\n",
    "            \n",
    "            # Build estimator\n",
    "            if model == 'ridge':\n",
    "                est = Ridge(alpha=opt_alpha[p,t])\n",
    "                est_name = 'ridge'\n",
    "            elif model == 'lasso':\n",
    "                est = Lasso(alpha=opt_alpha[p,t],\n",
    "                              random_state=random_seed + p, max_iter=1e7)\n",
    "                est_name = 'lasso'\n",
    "            elif model == 'elasticnet':\n",
    "                est = ElasticNet(alpha=opt_alpha[p,t], \n",
    "                                   l1_ratio=opt_l1[p,t],\n",
    "                                   random_state=random_seed + p, max_iter=1e7)\n",
    "                est_name = 'elasticnet'\n",
    "                \n",
    "            pipe = Pipeline([\n",
    "                ('sc', StandardScaler()),\n",
    "                (est_name, est)\n",
    "            ])\n",
    "\n",
    "            pipe.fit(X_train, y_train)\n",
    "            yhat = pipe.predict(X_test)\n",
    "\n",
    "            # Metrics\n",
    "            r2[p, t]   = r2_score(y_test, yhat)\n",
    "            mae[p, t]  = mean_absolute_error(y_test, yhat)\n",
    "            rmse[p, t] = mean_squared_error(y_test, yhat, squared=False)\n",
    "            pearson[p, t]  = np.corrcoef(y_test, yhat)[1, 0] if y_test.std() > 0 else np.nan\n",
    "            rho, _ = spearmanr(y_test, yhat)\n",
    "            spearman[p, t] = rho if y_test.std() > 0 else np.nan\n",
    "            \n",
    "\n",
    "        if (p + 1) % max(1, perm // 5) == 0:\n",
    "            print(f\"{p+1}/{perm} permutations\")\n",
    "\n",
    "    if date_prefix and descriptor:\n",
    "        # Save function\n",
    "        def save(outcome, array):\n",
    "            np.savetxt(os.path.join(output_dir, f'{date_prefix}_{descriptor}_{outcome}_NULL.csv'), array, delimiter=',')\n",
    "\n",
    "        save('r2', r2)\n",
    "        save('mae', mae)\n",
    "        save('rmse', rmse)\n",
    "        save('pearson', pearson)\n",
    "        save('spearman', spearman)\n",
    "\n",
    "\n",
    "    return {\n",
    "        'r2': r2, 'mae': mae, 'rmse': rmse, 'pearson': pearson, 'spearman': spearman,\n",
    "        'targets': target_names, 'model': model,\n",
    "        'config': {'perm': perm, 'test_size': test_size}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dda6b57-d00d-40b5-b4a0-189f563b5aaf",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf0f84-c6f3-42b3-be2a-32b14dffc656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final Y dataframe\n",
    "Y = df_scaled.drop(columns=[\"Age\", \"Sex\"]).values\n",
    "y_metric_names = df_scaled.drop(columns=[\"Age\", \"Sex\"]).columns.tolist()\n",
    "\n",
    "# Run model\n",
    "run_Null(x_res, Y, y_metric_names, model='elasticnet', perm=1000,\n",
    "                    date_prefix='230301',descriptor='elasticnet_partialcorr_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cac1f60-43e1-46aa-ae85-b4cf03f00b2a",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c94fe7a1-ce29-484f-b799-1993b9abb74d",
   "metadata": {},
   "source": [
    "means = np.nanmean(results['pearson'], axis=0)\n",
    "stds  = np.nanstd(results['pearson'], axis=0)\n",
    "names = results['targets']\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.errorbar(range(len(names)), means, yerr=stds, fmt='o-', capsize=4)\n",
    "plt.xticks(range(len(names)), names, rotation=30, ha='right')\n",
    "plt.ylabel(\"Pearson (mean ± SD)\", fontsize=14)\n",
    "plt.title(f\"Across {results['config']['perm']} permutations — {results['model']}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot first target's Pearson across permutations\n",
    "target_ix = 0\n",
    "series = results['pearson'][:, target_ix]\n",
    "plt.figure(figsize=(12,6)); plt.plot(series); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
